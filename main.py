import os
import re
import requests
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.constants import ParseMode
import asyncio
from datetime import datetime, timedelta
import html  # ğŸ’¡ [ì¶”ê°€] í…”ë ˆê·¸ë¨ HTML í¬ë§·íŒ…ì„ ìœ„í•´ import

BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = "@newsnissue"

PRESS = {
    "ì—°í•©ë‰´ìŠ¤": "001", "YTN": "052", "ì¡°ì„ ì¼ë³´": "023", "ì¤‘ì•™ì¼ë³´": "025", "ë™ì•„ì¼ë³´": "020",
    "êµ­ë¯¼ì¼ë³´": "005", "í•œêµ­ì¼ë³´": "469", "ì„œìš¸ì‹ ë¬¸": "081", "í•œê²¨ë ˆ": "028", "ê²½í–¥ì‹ ë¬¸": "032",
    "ë¬¸í™”ì¼ë³´": "021", "ë‰´ì‹œìŠ¤": "003", "ë‰´ìŠ¤1": "421", "KBS": "056", "MBC": "214",
    "SBS": "055", "JTBC": "437", "TVì¡°ì„ ": "448", "ë§¤ì¼ê²½ì œ": "009",
    "í•œêµ­ê²½ì œ": "015", "ì„œìš¸ê²½ì œ": "011", "í—¬ìŠ¤ì¡°ì„ ": "346"
}

ORDER = [
    "ì—°í•©ë‰´ìŠ¤", "YTN", "ì¡°ì„ ì¼ë³´", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "êµ­ë¯¼ì¼ë³´", "í•œêµ­ì¼ë³´",
    "ì„œìš¸ì‹ ë¬¸", "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸", "ë¬¸í™”ì¼ë³´", "ë‰´ì‹œìŠ¤", "ë‰´ìŠ¤1",
    "KBS", "MBC", "SBS", "JTBC", "TVì¡°ì„ ", "ë§¤ì¼ê²½ì œ",
    "í•œêµ­ê²½ì œ", "ì„œìš¸ê²½ì œ", "í—¬ìŠ¤ì¡°ì„ "
]

UA = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
}
TIMEOUT = 15


def normalize_link(href: str):
    if href.startswith("/"):
        return "https://n.news.naver.com" + href
    if href.startswith("http"):
        return href
    return "https://n.news.naver.com/" + href.lstrip("/")


def extract_real_sid_link(html_content, press_code, ranking_link):
    """HTML ë‚´ì—ì„œ sid=ê°€ í¬í•¨ëœ ì§„ì§œ ê¸°ì‚¬ ì£¼ì†Œë¥¼ ì°¾ì•„ëƒ„."""
    soup = BeautifulSoup(html_content, "html.parser")
    og = soup.find("meta", property="og:url")
    if og and og.get("content") and f"/article/{press_code}/" in og["content"]:
        return og["content"]
    canon = soup.find("link", rel="canonical")
    if canon and canon.get("href") and f"/article/{press_code}/" in canon["href"]:
        return canon["href"]
    sid_match = re.search(r"sid=(10[1-5])", html_content)
    art_match = re.search(r"/article/(\d{3})/(\d+)", ranking_link)
    if sid_match and art_match:
        sid = sid_match.group(1)
        p, aid = art_match.groups()
        return f"https://n.news.naver.com/mnews/article/{p}/{aid}?sid={sid}"
    return None


def get_real_article_link(ranking_link, press_code, title):
    """ë³¸ë¬¸ ì—´ì–´ì„œ sid ì£¼ì†Œ ì°¾ê¸° + ê²€ìƒ‰ fallback"""
    try:
        html_content = requests.get(ranking_link, headers=UA, timeout=8).text
        real = extract_real_sid_link(html_content, press_code, ranking_link)
        if real:
            return real
    except Exception:
        pass

    # fallback: ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
    try:
        search_url = f"https://search.naver.com/search.naver?where=news&query={requests.utils.quote(title)}"
        s_html = requests.get(search_url, headers=UA, timeout=8).text
        soup = BeautifulSoup(s_html, "html.parser")
        for a in soup.find_all("a", href=re.compile(rf"https://n\.news\.naver\.com/(mnews/)?article/{press_code}/\d+")):
            return a["href"]
    except Exception:
        pass

    return ranking_link


def clean_title(text: str) -> str:
    """ì œëª©ì—ì„œ ìˆœìœ„ ìˆ«ì/ì˜ìƒ/ì¡°íšŒìˆ˜ ë“± ë…¸ì´ì¦ˆ ì œê±°"""
    t = text.strip()
    
    # ğŸ’¡ [ìˆ˜ì •]
    # "40ëŒ€"ëŠ” ìœ ì§€í•˜ê³  "1." "2." ê°™ì€ ìˆœìœ„ ë²ˆí˜¸ë§Œ ì œê±°í•˜ê¸° ìœ„í•´
    # ìˆ«ì ë’¤ì— ì (.)ì´ ì˜¤ëŠ” ê²½ìš°ë§Œ ì‚­ì œí•˜ë„ë¡ ë³€ê²½
    t = re.sub(r"^\d+\.\s*", "", t) # ğŸ‘ˆ ì—¬ê¸°ê°€ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
    
    t = re.sub(r"\[[^\]]*ì˜ìƒ[^\]]*\]", "", t)
    t = re.sub(r"\([^)]*ì˜ìƒ[^)]*\)", "", t)
    t = re.sub(r"ì¡°íšŒìˆ˜\s*\d[\d,]*", "", t)
    t = re.sub(r"\s{2,}", " ", t)
    return t.strip()


def fetch_news_by_press(press_name, code, date_str):
    """ì–¸ë¡ ì‚¬ë³„ ìƒìœ„ ê¸°ì‚¬ 1~2ê°œ ê°€ì ¸ì˜¤ê¸°"""
    url = f"https://media.naver.com/press/{code}/ranking?type=popular&date={date_str}"
    try:
        html_content = requests.get(url, headers=UA, timeout=TIMEOUT).text
        soup = BeautifulSoup(html_content, "html.parser")

        # ğŸ’¡ [ê°œì„ ] ë” ì •í™•í•œ ì„ íƒìë¡œ ë³€ê²½
        # <ol class="list_ranking"> <li> ... </li> </ol> êµ¬ì¡°ë¥¼ íƒ€ê²Ÿ
        ranking_list = soup.select("ol.list_ranking li")
        if not ranking_list:
            # ë­í‚¹ ë¦¬ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°(êµ¬ì¡° ë³€ê²½ ë˜ëŠ” ê¸°ì‚¬ ì—†ìŒ)
            return press_name, []

        items, seen = [], set()

        # ğŸ’¡ [ê°œì„ ] find_all ëŒ€ì‹  CSS ì„ íƒìë¡œ ì°¾ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒ
        for item in ranking_list:
            a_tag = item.select_one(f'a[href*="/article/{code}/"]')
            if not a_tag:
                continue

            href = a_tag.get("href", "")
            m = re.search(rf"/article/{code}/(\d+)", href)
            if not m:
                continue
            
            aid = m.group(1)
            if aid in seen:
                continue
            seen.add(aid)

            # ğŸ’¡ [ê°œì„ ] ì œëª©ì„ <strong> íƒœê·¸ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½ (ë” ì•ˆì •ì )
            title_tag = item.select_one("strong.list_title")
            title_text = title_tag.get_text(strip=True) if title_tag else a_tag.get_text(strip=True)
            
            title = clean_title(title_text)
            if not title:
                continue

            link = normalize_link(href)
            
            # ğŸ’¡ [ê°œì„ ] ê¸°ì‚¬ ë³¸ë¬¸ ë§í¬ë¥¼ ì°¾ëŠ” ë¡œì§ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
            # real_link = get_real_article_link(link, code, title)
            # ë­í‚¹ í˜ì´ì§€ì˜ ë§í¬(link)ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë” ë¹ ë¦„)
            items.append({"title": title, "link": link})

            if len(items) >= 2: # ì–¸ë¡ ì‚¬ë³„ ìµœëŒ€ 2ê°œ
                break

        return press_name, items
    except Exception as e:
        print(f"âš ï¸ {press_name} ì˜¤ë¥˜: {e}")
        return press_name, []


def get_time_label():
    """í˜„ì¬ ì‹œê°ì— ë”°ë¼ ì˜¤ì „/ì˜¤í›„/ì €ë… ë°˜í™˜"""
    now = datetime.utcnow() + timedelta(hours=9)
    hour = now.hour
    if 8 <= hour < 12:
        return "ì˜¤ì „"
    elif 12 <= hour < 17:
        return "ì˜¤í›„"
    else:
        return "ì €ë…"


def build_message(news_dict):
    now = datetime.utcnow() + timedelta(hours=9)
    date_str = now.strftime("%m/%d")
    time_label = get_time_label()
    lines = [f"<b>{date_str} {time_label} ë§ì´ ë³¸ ë‰´ìŠ¤ ë¸Œë¦¬í•‘</b>\n"]

    for press in ORDER:
        arts = news_dict.get(press, [])
        if not arts:
            lines.append(f"<b>[{press}]</b> âš ï¸ ì¸ê¸° ê¸°ì‚¬ ì—†ìŒ")
            continue
        
        lines.append(f"<b>[{press}]</b>") # ğŸ’¡ [ê°œì„ ] ì–¸ë¡ ì‚¬ëª… ë³¼ë“œì²´
        for art in arts[:1]: # ì–¸ë¡ ì‚¬ë‹¹ 1ê°œë§Œ
            # ğŸ’¡ [ê°œì„ ] ì œëª©ì— <, > ë“±ì´ ìˆì–´ë„ ê¹¨ì§€ì§€ ì•Šê²Œ html.escape ì²˜ë¦¬
            safe_title = html.escape(art['title'])
            # ğŸ’¡ [ê°œì„ ] ì œëª©ì— ë§í¬ë¥¼ ë°”ë¡œ ê±°ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€ê²½
            lines.append(f'â€¢ <a href="{art["link"]}">{safe_title}</a>')
        lines.append("") # ì–¸ë¡ ì‚¬ë³„ í•œ ì¤„ ë„ìš°ê¸°

    return "\n".join(lines).rstrip()


async def send_to_telegram(text):
    if not BOT_TOKEN:
        print("âŒ TELEGRAM_BOT_TOKENì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    bot = Bot(token=BOT_TOKEN)
    try:
        await bot.send_message(
            chat_id=CHANNEL_ID,
            text=text,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True
        )
    except Exception as e:
        print(f"âŒ í…”ë ˆê·¸ë¨ ì „ì†¡ ì˜¤ë¥˜: {e}")


def main():
    date_str = (datetime.utcnow() + timedelta(hours=9)).strftime("%Y%m%d")
    print(f"{date_str} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    
    all_news = {}
    for name, code in PRESS.items():
        press, items = fetch_news_by_press(name, code, date_str)
        if items:
            print(f"  [âœ… {name}] {len(items)}ê°œ ìˆ˜ì§‘ ì„±ê³µ")
        all_news[press] = items
        
    print("ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ. ë©”ì‹œì§€ ìƒì„± ì¤‘...")
    msg = build_message(all_news)
    
    print("í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹œë„...")
    asyncio.run(send_to_telegram(msg))
    print("âœ… í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ.")


if __name__ == "__main__":
    main()

