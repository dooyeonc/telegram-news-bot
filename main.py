import os
import re
import requests
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.constants import ParseMode
import asyncio
from datetime import datetime, timedelta

BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = "@newsnissue"

PRESS = {
    "ì—°í•©ë‰´ìŠ¤": "001", "YTN": "052", "ì¡°ì„ ì¼ë³´": "023", "ì¤‘ì•™ì¼ë³´": "025", "ë™ì•„ì¼ë³´": "020",
    "êµ­ë¯¼ì¼ë³´": "005", "í•œêµ­ì¼ë³´": "469", "ì„œìš¸ì‹ ë¬¸": "081", "í•œê²¨ë ˆ": "028", "ê²½í–¥ì‹ ë¬¸": "032",
    "ë¬¸í™”ì¼ë³´": "021", "ë‰´ì‹œìŠ¤": "003", "ë‰´ìŠ¤1": "421", "KBS": "056", "MBC": "214",
    "SBS": "055", "JTBC": "437", "TVì¡°ì„ ": "448", "ë§¤ì¼ê²½ì œ": "009",
    "í•œêµ­ê²½ì œ": "015", "ì„œìš¸ê²½ì œ": "011", "í—¬ìŠ¤ì¡°ì„ ": "346"
}

ORDER = [
    "ì—°í•©ë‰´ìŠ¤", "YTN", "ì¡°ì„ ì¼ë³´", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "êµ­ë¯¼ì¼ë³´", "í•œêµ­ì¼ë³´",
    "ì„œìš¸ì‹ ë¬¸", "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸", "ë¬¸í™”ì¼ë³´", "ë‰´ì‹œìŠ¤", "ë‰´ìŠ¤1",
    "KBS", "MBC", "SBS", "JTBC", "TVì¡°ì„ ", "ë§¤ì¼ê²½ì œ",
    "í•œêµ­ê²½ì œ", "ì„œìš¸ê²½ì œ", "í—¬ìŠ¤ì¡°ì„ "
]

UA = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
}
TIMEOUT = 15


def normalize_link(href: str):
    if href.startswith("/"):
        return "https://n.news.naver.com" + href
    if href.startswith("http"):
        return href
    return "https://n.news.naver.com/" + href.lstrip("/")


def extract_real_sid_link(html, press_code, ranking_link):
    """HTML ë‚´ì—ì„œ sid=ê°€ í¬í•¨ëœ ì§„ì§œ ê¸°ì‚¬ ì£¼ì†Œë¥¼ ì°¾ì•„ëƒ„."""
    soup = BeautifulSoup(html, "html.parser")
    og = soup.find("meta", property="og:url")
    if og and og.get("content") and f"/article/{press_code}/" in og["content"]:
        return og["content"]
    canon = soup.find("link", rel="canonical")
    if canon and canon.get("href") and f"/article/{press_code}/" in canon["href"]:
        return canon["href"]
    sid_match = re.search(r"sid=(10[1-5])", html)
    art_match = re.search(r"/article/(\d{3})/(\d+)", ranking_link)
    if sid_match and art_match:
        sid = sid_match.group(1)
        p, aid = art_match.groups()
        return f"https://n.news.naver.com/mnews/article/{p}/{aid}?sid={sid}"
    return None


def get_real_article_link(ranking_link, press_code, title):
    """ë³¸ë¬¸ ì—´ì–´ì„œ sid ì£¼ì†Œ ì°¾ê¸° + ê²€ìƒ‰ fallback"""
    try:
        html = requests.get(ranking_link, headers=UA, timeout=8).text
        real = extract_real_sid_link(html, press_code, ranking_link)
        if real:
            return real
    except Exception:
        pass

    # fallback: ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
    try:
        search_url = f"https://search.naver.com/search.naver?where=news&query={requests.utils.quote(title)}"
        s_html = requests.get(search_url, headers=UA, timeout=8).text
        soup = BeautifulSoup(s_html, "html.parser")
        for a in soup.find_all("a", href=re.compile(rf"https://n\.news\.naver\.com/(mnews/)?article/{press_code}/\d+")):
            return a["href"]
    except Exception:
        pass

    return ranking_link


def clean_title(text: str) -> str:
    """ì œëª©ì—ì„œ ìˆœìœ„ ìˆ«ì/ì˜ìƒ/ì¡°íšŒìˆ˜ ë“± ë…¸ì´ì¦ˆ ì œê±°"""
    t = text.strip()
    # ğŸ”¹ ìˆœìœ„ ìˆ«ì(1. 2. 10. ë“±)ë§Œ ì œê±°, '40ëŒ€', '20ë…„' ë“±ì€ ìœ ì§€
    t = re.sub(r"^\d+\.\s*", "", t)
    # ğŸ”¹ [ì˜ìƒ], (ì˜ìƒ) ì œê±°
    t = re.sub(r"\[[^\]]*ì˜ìƒ[^\]]*\]", "", t)
    t = re.sub(r"\([^)]*ì˜ìƒ[^)]*\)", "", t)
    # ğŸ”¹ ì¡°íšŒìˆ˜ ì œê±°
    t = re.sub(r"ì¡°íšŒìˆ˜\s*\d[\d,]*", "", t)
    # ğŸ”¹ ì—¬ë¶„ì˜ ê³µë°± ì •ë¦¬
    t = re.sub(r"\s{2,}", " ", t)
    return t.strip()


def fetch_news_by_press(press_name, code, date_str):
    """ì–¸ë¡ ì‚¬ë³„ ìƒìœ„ ê¸°ì‚¬ 1~2ê°œ ê°€ì ¸ì˜¤ê¸°"""
    url = f"https://media.naver.com/press/{code}/ranking?type=popular&date={date_str}"
    try:
        html = requests.get(url, headers=UA, timeout=TIMEOUT).text
        soup = BeautifulSoup(html, "html.parser")

        a_tags = soup.find_all("a", href=re.compile(rf"/article/{code}/\d+"))
        items, seen = [], set()

        for a in a_tags:
            href = a.get("href", "")
            m = re.search(rf"/article/{code}/(\d+)", href)
            if not m:
                continue
            aid = m.group(1)
            if aid in seen:
                continue
            seen.add(aid)

            title = clean_title(a.get_text(strip=True))
            if not title:
                continue

            link = normalize_link(href)
            real_link = get_real_article_link(link, code, title)
            items.append({"title": title, "link": real_link})

            if len(items) >= 2:
                break

        return press_name, items
    except Exception as e:
        print(f"âš ï¸ {press_name} ì˜¤ë¥˜: {e}")
        return press_name, []


def get_time_label():
    """í˜„ì¬ ì‹œê°ì— ë”°ë¼ ì˜¤ì „/ì˜¤í›„/ì €ë… ë°˜í™˜"""
    now = datetime.utcnow() + timedelta(hours=9)
    hour = now.hour
    if 8 <= hour < 12:
        return "ì˜¤ì „"
    elif 12 <= hour < 17:
        return "ì˜¤í›„"
    else:
        return "ì €ë…"


def build_message(news_dict):
    now = datetime.utcnow() + timedelta(hours=9)
    date_str = now.strftime("%m/%d")
    time_label = get_time_label()
    lines = [f"<b>{date_str} {time_label} ë§ì´ ë³¸ ë‰´ìŠ¤ ë¸Œë¦¬í•‘</b>\n"]

    for press in ORDER:
        arts = news_dict.get(press, [])
        if not arts:
            lines.append(f"[{press}] âš ï¸ ì¸ê¸° ê¸°ì‚¬ ì—†ìŒ")
            continue
        lines.append(f"[{press}]")
        for art in arts[:1]:
            lines.append(f"{art['title']}")
            lines.append(art["link"])
        lines.append("")

    return "\n".join(lines).rstrip()


async def send_to_telegram(text):
    if not BOT_TOKEN:
        print("âŒ TELEGRAM_BOT_TOKENì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    bot = Bot(token=BOT_TOKEN)
    await bot.send_message(
        chat_id=CHANNEL_ID,
        text=text,
        parse_mode=ParseMode.HTML,
        disable_web_page_preview=True
    )


def main():
    date_str = (datetime.utcnow() + timedelta(hours=9)).strftime("%Y%m%d")
    all_news = {}
    for name, code in PRESS.items():
        press, items = fetch_news_by_press(name, code, date_str)
        all_news[press] = items
    msg = build_message(all_news)
    asyncio.run(send_to_telegram(msg))
    print("âœ… Sent")


if __name__ == "__main__":
    main()